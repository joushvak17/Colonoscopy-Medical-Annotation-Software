{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/yasserhessein/the-kvasir-dataset\n",
      "Downloading the-kvasir-dataset.zip to ./the-kvasir-dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.32G/2.32G [03:26<00:00, 12.1MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "# Download the dataset\n",
    "od.download(\"https://www.kaggle.com/datasets/yasserhessein/the-kvasir-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the paths for source, training, testing, and validation\n",
    "source_path = \"the-kvasir-dataset/kvasir-dataset-v2\"\n",
    "train_path = \"data/training\"\n",
    "test_path = \"data/testing\"\n",
    "validation_path = \"data/validation\"\n",
    "\n",
    "# Define the split ratios\n",
    "train_ratio = 0.7\n",
    "test_ratio = 0.2\n",
    "validation_ratio = 0.1\n",
    "\n",
    "# Create the directories\n",
    "for path in [train_path, test_path, validation_path]:\n",
    "    os.makedirs(path, exist_ok=True)    \n",
    "    \n",
    "# Process each class\n",
    "for class_name in os.listdir(source_path):\n",
    "    class_dir = os.path.join(source_path, class_name)\n",
    "    if os.path.isdir(class_dir):\n",
    "        # List all images\n",
    "        images = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))]\n",
    "        \n",
    "        # Split the dataset\n",
    "        train_val, test = train_test_split(images, test_size=test_ratio, random_state=42)\n",
    "        train, val = train_test_split(train_val, test_size=validation_ratio/(train_ratio+validation_ratio), random_state=42)\n",
    "        \n",
    "        # Define a function to copy files\n",
    "        def copy_files(filenames, dest_dir):\n",
    "            os.makedirs(dest_dir, exist_ok=True)\n",
    "            for f in filenames:\n",
    "                shutil.copy(f, dest_dir)\n",
    "                \n",
    "        # Copy the files\n",
    "        copy_files(train, os.path.join(train_path, class_name))\n",
    "        copy_files(test, os.path.join(test_path, class_name))\n",
    "        copy_files(val, os.path.join(validation_path, class_name))\n",
    "\n",
    "# Delete the downloaded dataset\n",
    "shutil.rmtree(\"the-kvasir-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  699\n",
      "Testing:  200\n",
      "Validation:  101\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the length of the training, testing, and validation datasets\n",
    "train_path = \"data/training/normal-z-line\"\n",
    "test_path = \"data/testing/normal-z-line\"\n",
    "validation_path = \"data/validation/normal-z-line\"\n",
    "\n",
    "print(\"Training: \", len(os.listdir(train_path)))\n",
    "print(\"Testing: \", len(os.listdir(test_path)))\n",
    "print(\"Validation: \", len(os.listdir(validation_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Import PyTorch and setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch.__version__)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modular/data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modular/data_setup.py\n",
    "\"\"\"\n",
    "Defines the functionality for creating PyTorch DataLoaders for the multi-class classification dataset.\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(train_dir: str, \n",
    "                       test_dir: str, \n",
    "                       transform: transforms.Compose, \n",
    "                       batch_size: int, \n",
    "                       num_workers: int=NUM_WORKERS):\n",
    "    \"\"\"Takes in a training and testing directory path and turns them into PyTorch DataLoaders.\n",
    "\n",
    "    Args:\n",
    "        train_dir (str): Path to training directory.\n",
    "        test_dir (str): Path to testing directory.\n",
    "        transform (transforms.Compose): Torchvision transforms to apply to the datasets.\n",
    "        batch_size (int): Number of samples per batch in each DataLoader.\n",
    "        num_workers (_type_): Number of workers per DataLoader. Currently set to os.cpu_count().\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Returns a tuple of (train_loader, test_loader, class_names). Where class_names is a dict of the target classes.\n",
    "    \"\"\"\n",
    "    # Use ImageFolder to create datasets\n",
    "    train_data = datasets.ImageFolder(root=train_dir,\n",
    "                                      transform=transform)\n",
    "    test_data = datasets.ImageFolder(root=test_dir,\n",
    "                                     transform=transform)\n",
    "    \n",
    "    # Get the class names\n",
    "    class_names = train_data.class_to_idx\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(dataset=train_data, \n",
    "                              batch_size=batch_size, \n",
    "                              num_workers=num_workers, \n",
    "                              shuffle=True,\n",
    "                              pin_memory=True)\n",
    "    \n",
    "    test_loader = DataLoader(dataset=test_data,\n",
    "                             batch_size=batch_size,\n",
    "                             num_workers=num_workers,\n",
    "                             shuffle=False,\n",
    "                             pin_memory=True)\n",
    "    \n",
    "    return train_loader, test_loader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modular/models/baseline_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modular/models/baseline_model.py\n",
    "\"\"\"\n",
    "Defines a PyTorch baseline model for multi-class classification.\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class BaseLine(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "        \"\"\"Defines a simple feedforward neural network for multi-class classification.\n",
    "\n",
    "        Args:\n",
    "            input_shape (int): Number of input channels.\n",
    "            hidden_units (int): Number of hidden units between layers.\n",
    "            output_shape (int): Number of output units.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_shape, hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units, output_shape),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modular/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modular/engine.py\n",
    "\"\"\"\n",
    "Defines functions for training and testing PyTorch models.\n",
    "\"\"\"\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"Turns the model into training mode and then runs through all the required training steps.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model to be trained.\n",
    "        dataloader (torch.utils.data.DataLoader): A DataLoader object for the training data.\n",
    "        loss_fn (torch.nn.Module): A PyTorch loss function to minimize.\n",
    "        optimizer (torch.optim.Optimizer): A PyTorch optimizer to update the model weights.\n",
    "        device (torch.device): A target device to send the data and model to.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: A tuple of the average loss and accuracy across all batches.\n",
    "    \"\"\"\n",
    "    # Put the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    #Setup the loss and accuracy\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # Iterate over the data\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send the data to the device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        y_pred = model(X)\n",
    "        \n",
    "        # Get the predictions\n",
    "        y_pred = model(X)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "        \n",
    "    # Adjust the metrics and get the avg loss and accuracy across all batches\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"Turns the model into evaluation mode and then runs through all the required testing steps.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model to be tested.\n",
    "        dataloader (torch.utils.data.DataLoader): A DataLoader object for the testing data.\n",
    "        loss_fn (torch.nn.Module): A PyTorch loss function to minimize.\n",
    "        device (torch.device): A target device to send the data and model to.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: A tuple of the average loss and accuracy across all batches.\n",
    "    \"\"\"\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Setup the loss and accuracy\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    # Turn on inference mode\n",
    "    with torch.inference_mode():\n",
    "        # Iterate over the data\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send the data to the device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "        \n",
    "            # Forward pass\n",
    "            y_pred = model(X)\n",
    "        \n",
    "            # Calculate the loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "        \n",
    "            # Calculate and accumulate accuracy metric across all batches\n",
    "            y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "            test_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "        \n",
    "    # Adjust the metrics and get the avg loss and accuracy across all batches\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_loader: torch.utils.data.DataLoader,\n",
    "          test_loader: torch.utils.data.DataLoader,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "          device: torch.device,\n",
    "          epochs: int,\n",
    "          patience: int = 5,\n",
    "          min_delta: float = 0.001) -> Dict[str, List[float]]:\n",
    "    \"\"\"Passes a model through training and testing steps for a specified number of epochs.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model to be trained and tested.\n",
    "        train_loader (torch.utils.data.DataLoader): A DataLoader object for the training data.\n",
    "        test_loader (torch.utils.data.DataLoader): A DataLoader object for the testing data.\n",
    "        loss_fn (torch.nn.Module): A PyTorch loss function to minimize.\n",
    "        optimizer (torch.optim.Optimizer): A PyTorch optimizer to update the model weights.\n",
    "        scheduler (torch.optim.lr_scheduler._LRScheduler): A PyTorch learning rate scheduler.\n",
    "        device (torch.device): A target device to send the data and model to.\n",
    "        epochs (int): The number of epochs to train the model for.\n",
    "        patience (int): The number of epochs to wait before early stopping.\n",
    "        min_delta (float): The minimum change in loss to be considered an improvement.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[float]]: A dictionary of lists containing the training and testing metrics.\n",
    "    \"\"\"\n",
    "    # Setup a dict to store results\n",
    "    results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n",
    "    \n",
    "    best_test_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # Perform a training step\n",
    "        train_loss, train_acc = train_step(model, train_loader, loss_fn, optimizer, device)\n",
    "        \n",
    "        # Perform a testing step\n",
    "        test_loss, test_acc = test_step(model, test_loader, loss_fn, device)\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step(test_loss)\n",
    "\n",
    "        # Append the metrics to the dict\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "        \n",
    "        # Print the metrics\n",
    "        print(f\"Epoch: {epoch+1}/{epochs} | Train Loss: {train_loss:.5f} | Train Acc: {train_acc:.5f} | Test Loss: {test_loss:.5f} | Test Acc: {test_acc:.5f}\")\n",
    "        \n",
    "        # Print the current learning rate\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print(f\"Current Learning Rate: {current_lr}\")\n",
    "\n",
    "        # Check for improvement\n",
    "        if test_loss < best_test_loss - min_delta:\n",
    "            best_test_loss = test_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "        # Check for early stopping\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Best model state loaded!\")\n",
    "        \n",
    "    # Return the results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modular/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modular/utils.py\n",
    "\"\"\"\n",
    "Defines functions that contain various utility functions for PyTorch model training and saving.\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "def save_model(model: torch.nn.Module,\n",
    "               target_dir: str,\n",
    "               model_name: str) -> None:\n",
    "    \"\"\"Save a PyTorch model to a specified directory.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model to be saved.\n",
    "        target_dir (str): The directory path to save the model.\n",
    "        model_name (str): The name of the model file.\n",
    "    \"\"\"\n",
    "    # Create the target directory\n",
    "    Path(target_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create model save path\n",
    "    assert model_name.endswith(\".pt\") or model_name.endswith(\".pth\"), \"Model name must end with .pt or .pth\"\n",
    "    model_save_path = Path(target_dir) / model_name\n",
    "    \n",
    "    # Save the model\n",
    "    print(f\"Saving model to: {model_save_path}\")\n",
    "    torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modular/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modular/train.py\n",
    "\"\"\"\n",
    "Defines the training script for the PyTorch model.\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "\n",
    "import inspect\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import importlib.util\n",
    "\n",
    "import sys\n",
    "# Adjust the path to include the modular directory and where the scripts are located\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "sys.path.append(\"modular\")\n",
    "sys.path.append(\"modular/models\")\n",
    "\n",
    "import data_setup, engine, utils\n",
    "\n",
    "# Function to list available models\n",
    "def list_models():\n",
    "    models_dir = os.path.join(script_dir, \"models\")\n",
    "    model_files = [f for f in os.listdir(models_dir) if f.endswith(\".py\")]\n",
    "    model_files = [\"models/\" + f for f in model_files]\n",
    "    return \", \".join(model_files)\n",
    "\n",
    "# Create ArgumentParser object\n",
    "parser = argparse.ArgumentParser(description=\"Train a PyTorch multiclassification model on the colonoscopy dataset.\")\n",
    "\n",
    "# Add the arguments\n",
    "parser.add_argument(\"--num_epochs\", type=int, default=20, help=\"Number of epochs to train the model. Default is 20.\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=32, help=\"Number of samples per batch. Default is 32.\")\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=0.005, help=\"Learning rate for the optimizer. Default is 0.005.\")\n",
    "parser.add_argument(\"--hidden_units\", type=int, default=10, help=\"Number of hidden units in the model. Default is 10. Not needed for transfer learning models.\")\n",
    "\n",
    "parser.add_argument(\"--model_path\", type=str, required=True, help=f\"Path to the model file. Argument is required. Available models: {list_models()}\")\n",
    "# TODO: Check to see if this argument is needed\n",
    "# parser.add_argument(\"--transfer_learning\", action=\"store_true\", help=\"Indicate if the model is a transfer learning model.\")\n",
    "\n",
    "# Parse the arguments\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Setup hyperparameters\n",
    "NUM_EPOCHS = args.num_epochs\n",
    "BATCH_SIZE = args.batch_size\n",
    "LEARNING_RATE = args.learning_rate\n",
    "HIDDEN_UNITS = args.hidden_units\n",
    "\n",
    "# Define the mapping of model names to their torchvision equivalents and default transformations\n",
    "TRANSFER_LEARNING_MODELS = {\n",
    "    \"vgg19_model\": models.VGG19_Weights.DEFAULT\n",
    "}\n",
    "\n",
    "# Import the specified model\n",
    "model_script_path = os.path.join(script_dir, args.model_path)\n",
    "spec = importlib.util.spec_from_file_location(\"model_module\", model_script_path)\n",
    "model_module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(model_module)\n",
    "\n",
    "# Define a function that returns the correct transformation\n",
    "def get_transforms(model_name):\n",
    "    if model_name in TRANSFER_LEARNING_MODELS:\n",
    "        weights = TRANSFER_LEARNING_MODELS[model_name]\n",
    "        transfer_transform = weights.transforms()\n",
    "        return transfer_transform\n",
    "    else:\n",
    "        default_transform = transforms.Compose([transforms.Resize((224, 224)), \n",
    "                                             transforms.ToTensor()])\n",
    "        return default_transform\n",
    "\n",
    "# Get the transformation based on the model name\n",
    "model_name = os.path.basename(args.model_path).replace(\".py\", \"\")\n",
    "data_transform = get_transforms(model_name)\n",
    "\n",
    "model_class = None\n",
    "for name, obj in inspect.getmembers(model_module):\n",
    "    if inspect.isclass(obj):\n",
    "        model_class = obj\n",
    "        break\n",
    "\n",
    "if model_class is None:\n",
    "    raise ValueError(f\"Model class not found in {model_script_path}\")\n",
    "\n",
    "# Setup the directories\n",
    "train_dir = \"data/training\"\n",
    "test_dir = \"data/testing\"\n",
    "\n",
    "# Setup target device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create the DataLoaders using data_setup.py\n",
    "train_loader, test_loader, class_names = data_setup.create_dataloaders(train_dir, \n",
    "                                                                       test_dir, \n",
    "                                                                       data_transform, \n",
    "                                                                       BATCH_SIZE)\n",
    "\n",
    "# Create the model\n",
    "model = model_class(input_shape=3, \n",
    "                    hidden_units=HIDDEN_UNITS, \n",
    "                    output_shape=len(class_names)).to(device)\n",
    "\n",
    "# Set the loss function and optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "# Start training the model using engine.py\n",
    "engine.train(model, train_loader, test_loader, loss_fn, optimizer, device, NUM_EPOCHS)\n",
    "\n",
    "# Prompt the user to save the model\n",
    "save_prompt = input(\"Do you want to save the model? (yes/no): \").lower()\n",
    "if save_prompt == \"yes\":\n",
    "    model_name = input(\"Enter the model name (without extension): \")\n",
    "    utils.save_model(model, \"saved_models\", model_name + \".pth\")\n",
    "else: \n",
    "    print(\"Okay model will not be saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1cd5fadac7e4ae58b2a878c1d4c151b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30 | Train Loss: 1.47666 | Train Acc: 0.57262 | Test Loss: 0.58629 | Test Acc: 0.76250\n",
      "Current Learning Rate: 0.005\n",
      "Epoch: 2/30 | Train Loss: 0.76340 | Train Acc: 0.74940 | Test Loss: 0.56052 | Test Acc: 0.81812\n",
      "Current Learning Rate: 0.005\n",
      "Epoch: 3/30 | Train Loss: 0.63440 | Train Acc: 0.79536 | Test Loss: 0.60553 | Test Acc: 0.79937\n",
      "Current Learning Rate: 0.005\n",
      "Epoch: 4/30 | Train Loss: 0.51551 | Train Acc: 0.83137 | Test Loss: 0.51371 | Test Acc: 0.83813\n",
      "Current Learning Rate: 0.005\n",
      "Epoch: 5/30 | Train Loss: 0.49215 | Train Acc: 0.84137 | Test Loss: 0.51639 | Test Acc: 0.81812\n",
      "Current Learning Rate: 0.005\n",
      "Epoch: 6/30 | Train Loss: 0.37484 | Train Acc: 0.87071 | Test Loss: 0.42639 | Test Acc: 0.85375\n",
      "Current Learning Rate: 0.005\n",
      "Epoch: 7/30 | Train Loss: 0.36799 | Train Acc: 0.87952 | Test Loss: 0.52201 | Test Acc: 0.83250\n",
      "Current Learning Rate: 0.005\n",
      "Epoch: 8/30 | Train Loss: 0.38108 | Train Acc: 0.88226 | Test Loss: 0.52168 | Test Acc: 0.85188\n",
      "Current Learning Rate: 0.005\n",
      "Epoch: 9/30 | Train Loss: 0.28646 | Train Acc: 0.90333 | Test Loss: 0.50400 | Test Acc: 0.87125\n",
      "Current Learning Rate: 0.005\n",
      "Epoch: 10/30 | Train Loss: 0.28881 | Train Acc: 0.91244 | Test Loss: 0.45599 | Test Acc: 0.86625\n",
      "Current Learning Rate: 0.0005\n",
      "Epoch: 11/30 | Train Loss: 0.18070 | Train Acc: 0.93750 | Test Loss: 0.41193 | Test Acc: 0.88187\n",
      "Current Learning Rate: 0.0005\n",
      "Epoch: 12/30 | Train Loss: 0.14866 | Train Acc: 0.94708 | Test Loss: 0.39615 | Test Acc: 0.88438\n",
      "Current Learning Rate: 0.0005\n",
      "Epoch: 13/30 | Train Loss: 0.13222 | Train Acc: 0.95321 | Test Loss: 0.41211 | Test Acc: 0.87938\n",
      "Current Learning Rate: 0.0005\n",
      "Epoch: 14/30 | Train Loss: 0.11183 | Train Acc: 0.95923 | Test Loss: 0.44235 | Test Acc: 0.88375\n",
      "Current Learning Rate: 0.0005\n",
      "Epoch: 15/30 | Train Loss: 0.10375 | Train Acc: 0.96161 | Test Loss: 0.44692 | Test Acc: 0.87750\n",
      "Current Learning Rate: 0.0005\n",
      "Epoch: 16/30 | Train Loss: 0.10118 | Train Acc: 0.96196 | Test Loss: 0.45987 | Test Acc: 0.88375\n",
      "Current Learning Rate: 5e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimeit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m default_timer \u001b[38;5;28;01mas\u001b[39;00m timer\n\u001b[1;32m     42\u001b[0m start_timer \u001b[38;5;241m=\u001b[39m timer()\n\u001b[0;32m---> 44\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodular\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m                               \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPATIENCE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmin_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m end_timer \u001b[38;5;241m=\u001b[39m timer()\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime taken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_timer\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_timer\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Colonoscopy-Medical-Annotation-Software/Colonoscopy Medical Annotation Project/modular/engine.py:150\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, loss_fn, optimizer, scheduler, device, epochs, patience, min_delta)\u001b[0m\n\u001b[1;32m    147\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m train_step(model, train_loader, loss_fn, optimizer, device)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Perform a testing step\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# Step the scheduler\u001b[39;00m\n\u001b[1;32m    153\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(test_loss)\n",
      "File \u001b[0;32m~/Colonoscopy-Medical-Annotation-Software/Colonoscopy Medical Annotation Project/modular/engine.py:99\u001b[0m, in \u001b[0;36mtest_step\u001b[0;34m(model, dataloader, loss_fn, device)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Calculate the loss\u001b[39;00m\n\u001b[1;32m     98\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y)\n\u001b[0;32m---> 99\u001b[0m test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Calculate and accumulate accuracy metric across all batches\u001b[39;00m\n\u001b[1;32m    102\u001b[0m y_pred_class \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(torch\u001b[38;5;241m.\u001b[39msoftmax(y_pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Try to run the VGG19 model through the notebook\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import modular.data_setup, modular.engine\n",
    "\n",
    "NUM_EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.005\n",
    "PATIENCE = 5\n",
    "\n",
    "weights = models.VGG19_Weights.DEFAULT\n",
    "data_transform = weights.transforms()\n",
    "\n",
    "train_dir = \"data/training\"\n",
    "test_dir = \"data/testing\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_loader, test_loader, class_names = modular.data_setup.create_dataloaders(train_dir, \n",
    "                                                                               test_dir,\n",
    "                                                                               data_transform,\n",
    "                                                                               BATCH_SIZE)\n",
    "\n",
    "model = models.vgg19(weights=weights).to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.features[-5:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "num_ftrs = model.classifier[6].in_features\n",
    "model.classifier[6] = torch.nn.Linear(num_ftrs, len(class_names)).to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=3)\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "start_timer = timer()\n",
    "\n",
    "results = modular.engine.train(model, \n",
    "                               train_loader, \n",
    "                               test_loader, \n",
    "                               loss_fn, \n",
    "                               optimizer, \n",
    "                               scheduler,\n",
    "                               device, \n",
    "                               NUM_EPOCHS, \n",
    "                               patience=PATIENCE, \n",
    "                               min_delta=0.001)\n",
    "\n",
    "end_timer = timer()\n",
    "print(f\"Time taken: {end_timer - start_timer:.3f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ThirdProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
