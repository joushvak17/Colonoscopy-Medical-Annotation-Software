{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "# Download the dataset\n",
    "od.download(\"https://www.kaggle.com/datasets/yasserhessein/the-kvasir-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the paths for source, training, testing, and validation\n",
    "source_path = \"the-kvasir-dataset/kvasir-dataset-v2\"\n",
    "train_path = \"data/training\"\n",
    "test_path = \"data/testing\"\n",
    "validation_path = \"data/validation\"\n",
    "\n",
    "# Define the split ratios\n",
    "train_ratio = 0.7\n",
    "test_ratio = 0.2\n",
    "validation_ratio = 0.1\n",
    "\n",
    "# Create the directories\n",
    "for path in [train_path, test_path, validation_path]:\n",
    "    os.makedirs(path, exist_ok=True)    \n",
    "    \n",
    "# Process each class\n",
    "for class_name in os.listdir(source_path):\n",
    "    class_dir = os.path.join(source_path, class_name)\n",
    "    if os.path.isdir(class_dir):\n",
    "        # List all images\n",
    "        images = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))]\n",
    "        \n",
    "        # Split the dataset\n",
    "        train_val, test = train_test_split(images, test_size=test_ratio, random_state=42)\n",
    "        train, val = train_test_split(train_val, test_size=validation_ratio/(train_ratio+validation_ratio), random_state=42)\n",
    "        \n",
    "        # Define a function to copy files\n",
    "        def copy_files(filenames, dest_dir):\n",
    "            os.makedirs(dest_dir, exist_ok=True)\n",
    "            for f in filenames:\n",
    "                shutil.copy(f, dest_dir)\n",
    "                \n",
    "        # Copy the files\n",
    "        copy_files(train, os.path.join(train_path, class_name))\n",
    "        copy_files(test, os.path.join(test_path, class_name))\n",
    "        copy_files(val, os.path.join(validation_path, class_name))\n",
    "\n",
    "# Delete the downloaded dataset\n",
    "shutil.rmtree(\"the-kvasir-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  699\n",
      "Testing:  200\n",
      "Validation:  101\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the length of the training, testing, and validation datasets\n",
    "train_path = \"data/training/normal-z-line\"\n",
    "test_path = \"data/testing/normal-z-line\"\n",
    "validation_path = \"data/validation/normal-z-line\"\n",
    "\n",
    "print(\"Training: \", len(os.listdir(train_path)))\n",
    "print(\"Testing: \", len(os.listdir(test_path)))\n",
    "print(\"Validation: \", len(os.listdir(validation_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Import PyTorch and setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch.__version__)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modular/data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modular/data_setup.py\n",
    "\"\"\"\n",
    "Defines the functionality for creating PyTorch DataLoaders for the multi-class classification dataset.\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(train_dir: str, \n",
    "                       test_dir: str, \n",
    "                       train_transform: transforms.Compose,\n",
    "                       test_transform: transforms.Compose,\n",
    "                       batch_size: int, \n",
    "                       num_workers: int=NUM_WORKERS):\n",
    "    \"\"\"Takes in a training and testing directory path and turns them into PyTorch DataLoaders.\n",
    "\n",
    "    Args:\n",
    "        train_dir (str): Path to training directory.\n",
    "        test_dir (str): Path to testing directory.\n",
    "        train_transform (transforms.Compose): Torchvision transforms to apply to the training dataset.\n",
    "        test_transform (transforms.Compose): Torchvision transforms to apply to the testing dataset.\n",
    "        batch_size (int): Number of samples per batch in each DataLoader.\n",
    "        num_workers (int): Number of workers per DataLoader. Currently set to os.cpu_count().\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Returns a tuple of (train_loader, test_loader, class_names). Where class_names is a dict of the target classes.\n",
    "    \"\"\"\n",
    "    # Use ImageFolder to create datasets\n",
    "    train_data = datasets.ImageFolder(root=train_dir,\n",
    "                                      transform=train_transform)\n",
    "    test_data = datasets.ImageFolder(root=test_dir,\n",
    "                                     transform=test_transform)\n",
    "    \n",
    "    # Get the class names\n",
    "    class_names = train_data.class_to_idx\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(dataset=train_data, \n",
    "                              batch_size=batch_size, \n",
    "                              num_workers=num_workers, \n",
    "                              shuffle=True,\n",
    "                              pin_memory=True)\n",
    "    \n",
    "    test_loader = DataLoader(dataset=test_data,\n",
    "                             batch_size=batch_size,\n",
    "                             num_workers=num_workers,\n",
    "                             shuffle=False,\n",
    "                             pin_memory=True)\n",
    "    \n",
    "    return train_loader, test_loader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modular/models/baseline_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modular/models/baseline_model.py\n",
    "\"\"\"\n",
    "Defines a PyTorch baseline model for multi-class classification.\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class BaseLine(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "        \"\"\"Defines a simple feedforward neural network for multi-class classification.\n",
    "\n",
    "        Args:\n",
    "            input_shape (int): Number of input channels.\n",
    "            hidden_units (int): Number of hidden units between layers.\n",
    "            output_shape (int): Number of output units.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_shape, hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units, output_shape),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modular/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modular/engine.py\n",
    "\"\"\"\n",
    "Defines functions for training and testing PyTorch models.\n",
    "\"\"\"\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"Turns the model into training mode and then runs through all the required training steps.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model to be trained.\n",
    "        dataloader (torch.utils.data.DataLoader): A DataLoader object for the training data.\n",
    "        loss_fn (torch.nn.Module): A PyTorch loss function to minimize.\n",
    "        optimizer (torch.optim.Optimizer): A PyTorch optimizer to update the model weights.\n",
    "        device (torch.device): A target device to send the data and model to.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: A tuple of the average loss and accuracy across all batches.\n",
    "    \"\"\"\n",
    "    # Put the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    #Setup the loss and accuracy\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # Iterate over the data\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send the data to the device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        y_pred = model(X)\n",
    "        \n",
    "        # Get the predictions\n",
    "        y_pred = model(X)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "        \n",
    "    # Adjust the metrics and get the avg loss and accuracy across all batches\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"Turns the model into evaluation mode and then runs through all the required testing steps.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model to be tested.\n",
    "        dataloader (torch.utils.data.DataLoader): A DataLoader object for the testing data.\n",
    "        loss_fn (torch.nn.Module): A PyTorch loss function to minimize.\n",
    "        device (torch.device): A target device to send the data and model to.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: A tuple of the average loss and accuracy across all batches.\n",
    "    \"\"\"\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Setup the loss and accuracy\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    # Turn on inference mode\n",
    "    with torch.inference_mode():\n",
    "        # Iterate over the data\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send the data to the device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "        \n",
    "            # Forward pass\n",
    "            y_pred = model(X)\n",
    "        \n",
    "            # Calculate the loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "        \n",
    "            # Calculate and accumulate accuracy metric across all batches\n",
    "            y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "            test_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "        \n",
    "    # Adjust the metrics and get the avg loss and accuracy across all batches\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_loader: torch.utils.data.DataLoader,\n",
    "          test_loader: torch.utils.data.DataLoader,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          scheduler: torch.optim.lr_scheduler,\n",
    "          device: torch.device,\n",
    "          epochs: int,\n",
    "          patience: int = 5,\n",
    "          min_delta: float = 0.001) -> Dict[str, List[float]]:\n",
    "    \"\"\"Passes a model through training and testing steps for a specified number of epochs.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model to be trained and tested.\n",
    "        train_loader (torch.utils.data.DataLoader): A DataLoader object for the training data.\n",
    "        test_loader (torch.utils.data.DataLoader): A DataLoader object for the testing data.\n",
    "        loss_fn (torch.nn.Module): A PyTorch loss function to minimize.\n",
    "        optimizer (torch.optim.Optimizer): A PyTorch optimizer to update the model weights.\n",
    "        scheduler (torch.optim.lr_scheduler._LRScheduler): A PyTorch learning rate scheduler.\n",
    "        device (torch.device): A target device to send the data and model to.\n",
    "        epochs (int): The number of epochs to train the model for.\n",
    "        patience (int): The number of epochs to wait before early stopping.\n",
    "        min_delta (float): The minimum change in loss to be considered an improvement.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[float]]: A dictionary of lists containing the training and testing metrics.\n",
    "    \"\"\"\n",
    "    # Setup a dict to store results\n",
    "    results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n",
    "    \n",
    "    best_test_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # Perform a training step\n",
    "        train_loss, train_acc = train_step(model, train_loader, loss_fn, optimizer, device)\n",
    "        \n",
    "        # Perform a testing step\n",
    "        test_loss, test_acc = test_step(model, test_loader, loss_fn, device)\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step(test_loss)\n",
    "\n",
    "        # Append the metrics to the dict\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "        \n",
    "        # Print the metrics\n",
    "        print(f\"Epoch: {epoch+1}/{epochs} | Train Loss: {train_loss:.5f} | Train Acc: {train_acc:.5f} | Test Loss: {test_loss:.5f} | Test Acc: {test_acc:.5f}\")\n",
    "        \n",
    "        # Print the current learning rate\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print(f\"Current Learning Rate: {current_lr}\")\n",
    "\n",
    "        # Check for improvement\n",
    "        if test_loss < best_test_loss - min_delta:\n",
    "            best_test_loss = test_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "        # Check for early stopping\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Best model state loaded!\")\n",
    "        \n",
    "    # Return the results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modular/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modular/utils.py\n",
    "\"\"\"\n",
    "Defines functions that contain various utility functions for PyTorch model training and saving.\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "def save_model(model: torch.nn.Module,\n",
    "               target_dir: str,\n",
    "               model_name: str) -> None:\n",
    "    \"\"\"Save a PyTorch model to a specified directory.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model to be saved.\n",
    "        target_dir (str): The directory path to save the model.\n",
    "        model_name (str): The name of the model file.\n",
    "    \"\"\"\n",
    "    # Create the target directory\n",
    "    Path(target_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create model save path\n",
    "    assert model_name.endswith(\".pt\") or model_name.endswith(\".pth\"), \"Model name must end with .pt or .pth\"\n",
    "    model_save_path = Path(target_dir) / model_name\n",
    "    \n",
    "    # Save the model\n",
    "    print(f\"Saving model to: {model_save_path}\")\n",
    "    torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modular/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modular/train.py\n",
    "\"\"\"\n",
    "Defines the training script for the PyTorch model.\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "\n",
    "import inspect\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import importlib.util\n",
    "\n",
    "import sys\n",
    "# Adjust the path to include the modular directory and where the scripts are located\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "sys.path.append(\"modular\")\n",
    "sys.path.append(\"modular/models\")\n",
    "\n",
    "import data_setup, engine, utils\n",
    "\n",
    "# Function to list available models\n",
    "def list_models():\n",
    "    models_dir = os.path.join(script_dir, \"models\")\n",
    "    model_files = [f for f in os.listdir(models_dir) if f.endswith(\".py\")]\n",
    "    model_files = [\"models/\" + f for f in model_files]\n",
    "    return \", \".join(model_files)\n",
    "\n",
    "# Create ArgumentParser object\n",
    "parser = argparse.ArgumentParser(description=\"Train a PyTorch multiclassification model on the colonoscopy dataset.\")\n",
    "\n",
    "# Add the arguments\n",
    "parser.add_argument(\"--num_epochs\", type=int, default=20, help=\"Number of epochs to train the model. Default is 20.\")\n",
    "parser.add_argument(\"--patience\", type=int, default=5, help=\"Number of epochs to wait before early stopping. Default is 5.\")\n",
    "parser.add_argument(\"--min_delta\", type=float, default=0.001, help=\"Minimum change in loss to be considered an improvement. Default is 0.001.\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=32, help=\"Number of samples per batch. Default is 32.\")\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=0.001, help=\"Learning rate for the optimizer. Default is 0.001.\")\n",
    "parser.add_argument(\"--hidden_units\", type=int, default=10, help=\"Number of hidden units in the model. Default is 10. Not needed for transfer learning models.\")\n",
    "parser.add_argument(\"--model_path\", type=str, required=True, help=f\"Path to the model file. Argument is required. Available models: {list_models()}\")\n",
    "\n",
    "\n",
    "# Parse the arguments\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Setup hyperparameters\n",
    "NUM_EPOCHS = args.num_epochs\n",
    "BATCH_SIZE = args.batch_size\n",
    "LEARNING_RATE = args.learning_rate\n",
    "HIDDEN_UNITS = args.hidden_units\n",
    "PATIENCE = args.patience\n",
    "MIN_DELTA = args.min_delta\n",
    "\n",
    "# Define the mapping of model names to their torchvision equivalents and default transformations\n",
    "TRANSFER_LEARNING_MODELS = {\n",
    "    \"vgg19_model\": models.VGG19_Weights.DEFAULT\n",
    "}\n",
    "\n",
    "# Import the specified model\n",
    "model_script_path = os.path.join(script_dir, args.model_path)\n",
    "spec = importlib.util.spec_from_file_location(\"model_module\", model_script_path)\n",
    "model_module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(model_module)\n",
    "\n",
    "def get_transforms(model_name):\n",
    "    if model_name in TRANSFER_LEARNING_MODELS:\n",
    "        weights = TRANSFER_LEARNING_MODELS[model_name]\n",
    "        base_transform = weights.transforms()\n",
    "        \n",
    "        # Add data augmentation for training\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "            base_transform\n",
    "        ])\n",
    "        \n",
    "        # Use only the base transform for testing\n",
    "        test_transform = base_transform\n",
    "        \n",
    "    else:\n",
    "        # Default transforms if the model is not in TRANSFER_LEARNING_MODELS\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        test_transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    return train_transform, test_transform\n",
    "\n",
    "# Get the transformation based on the model name\n",
    "model_name = os.path.basename(args.model_path).replace(\".py\", \"\")\n",
    "train_transform, test_transform = get_transforms(model_name)\n",
    "\n",
    "model_class = None\n",
    "for name, obj in inspect.getmembers(model_module):\n",
    "    if inspect.isclass(obj):\n",
    "        model_class = obj\n",
    "        break\n",
    "\n",
    "if model_class is None:\n",
    "    raise ValueError(f\"Model class not found in {model_script_path}\")\n",
    "\n",
    "# Setup the directories\n",
    "train_dir = \"data/training\"\n",
    "test_dir = \"data/testing\"\n",
    "\n",
    "# Setup target device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create the DataLoaders using data_setup.py\n",
    "train_loader, test_loader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    train_transform=train_transform,\n",
    "    test_transform=test_transform,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Create the model\n",
    "if model_name in TRANSFER_LEARNING_MODELS:\n",
    "    model = model_class(output_shape=len(class_names), device=device).to(device)\n",
    "else:\n",
    "    model = model_class(input_shape=3,\n",
    "                        hidden_units=HIDDEN_UNITS,\n",
    "                        output_shape=len(class_names)).to(device)\n",
    "\n",
    "# Set the loss function and optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# TODO: Figure out optimal optimizer and scheduler to use and the parameters to pass\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "\n",
    "# Start training the model using engine.py\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "start_timer = timer()\n",
    "\n",
    "engine.train(model=model,\n",
    "train_loader=train_loader, \n",
    "test_loader=test_loader, \n",
    "loss_fn=loss_fn, \n",
    "optimizer=optimizer, \n",
    "scheduler=scheduler, \n",
    "device=device, \n",
    "epochs=NUM_EPOCHS,\n",
    "patience=PATIENCE,\n",
    "min_delta=MIN_DELTA)\n",
    "\n",
    "end_timer = timer()\n",
    "\n",
    "print(f\"Training took: {end_timer - start_timer} seconds\")\n",
    "\n",
    "# Prompt the user to save the model\n",
    "save_prompt = input(\"Do you want to save the model? (yes/no): \").lower()\n",
    "if save_prompt == \"yes\":\n",
    "    model_name = input(\"Enter the model name (without extension): \")\n",
    "    utils.save_model(model, \"saved_models\", model_name + \".pth\")\n",
    "else: \n",
    "    print(\"Okay model will not be saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ThirdProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
