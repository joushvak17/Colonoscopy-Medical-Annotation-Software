{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/yasserhessein/the-kvasir-dataset\n",
      "Downloading the-kvasir-dataset.zip to ./the-kvasir-dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.32G/2.32G [00:36<00:00, 68.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "\n",
    "# Download the dataset\n",
    "od.download(\"https://www.kaggle.com/datasets/yasserhessein/the-kvasir-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the paths for source, training, testing, and validation\n",
    "source_path = \"the-kvasir-dataset/kvasir-dataset-v2\"\n",
    "train_path = \"data/training\"\n",
    "test_path = \"data/testing\"\n",
    "validation_path = \"data/validation\"\n",
    "\n",
    "# Define the split ratios\n",
    "train_ratio = 0.7\n",
    "test_ratio = 0.2\n",
    "validation_ratio = 0.1\n",
    "\n",
    "# Create the directories\n",
    "for path in [train_path, test_path, validation_path]:\n",
    "    os.makedirs(path, exist_ok=True)    \n",
    "    \n",
    "# Process each class\n",
    "for class_name in os.listdir(source_path):\n",
    "    class_dir = os.path.join(source_path, class_name)\n",
    "    if os.path.isdir(class_dir):\n",
    "        # List all images\n",
    "        images = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))]\n",
    "        \n",
    "        # Split the dataset\n",
    "        train_val, test = train_test_split(images, test_size=test_ratio, random_state=42)\n",
    "        train, val = train_test_split(train_val, test_size=validation_ratio/(train_ratio+validation_ratio), random_state=42)\n",
    "        \n",
    "        # Define a function to copy files\n",
    "        def copy_files(filenames, dest_dir):\n",
    "            os.makedirs(dest_dir, exist_ok=True)\n",
    "            for f in filenames:\n",
    "                shutil.copy(f, dest_dir)\n",
    "                \n",
    "        # Copy the files\n",
    "        copy_files(train, os.path.join(train_path, class_name))\n",
    "        copy_files(test, os.path.join(test_path, class_name))\n",
    "        copy_files(val, os.path.join(validation_path, class_name))\n",
    "\n",
    "# Delete the downloaded dataset\n",
    "shutil.rmtree(\"the-kvasir-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  699\n",
      "Testing:  200\n",
      "Validation:  101\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the length of the training, testing, and validation datasets\n",
    "train_path = \"data/training/normal-z-line\"\n",
    "test_path = \"data/testing/normal-z-line\"\n",
    "validation_path = \"data/validation/normal-z-line\"\n",
    "\n",
    "print(\"Training: \", len(os.listdir(train_path)))\n",
    "print(\"Testing: \", len(os.listdir(test_path)))\n",
    "print(\"Validation: \", len(os.listdir(validation_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Import PyTorch and setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch.__version__)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset:\n",
      " Dataset ImageFolder\n",
      "    Number of datapoints: 5592\n",
      "    Root location: data/training\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "           )\n",
      "Test dataset:\n",
      " Dataset ImageFolder\n",
      "    Number of datapoints: 1600\n",
      "    Root location: data/testing\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_path = \"data/training\"\n",
    "test_path = \"data/testing\"\n",
    "\n",
    "# Create simple transformations\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Use ImageFolder to create datasets\n",
    "train_dataset = datasets.ImageFolder(root=train_path, \n",
    "                                     transform=data_transform,\n",
    "                                     target_transform=None)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root=test_path,\n",
    "                                    transform = data_transform)\n",
    "\n",
    "print(f\"Train dataset:\\n {train_dataset}\\nTest dataset:\\n {test_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dyed-lifted-polyps': 0, 'dyed-resection-margins': 1, 'esophagitis': 2, 'normal-cecum': 3, 'normal-pylorus': 4, 'normal-z-line': 5, 'polyps': 6, 'ulcerative-colitis': 7}\n"
     ]
    }
   ],
   "source": [
    "# Get the class names as a dict\n",
    "class_names = train_dataset.class_to_idx\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5592, 1600)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the length of the datasets\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7fc51f039ac0>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7fc51f0396d0>\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=32, \n",
    "                          num_workers=NUM_WORKERS,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=32,\n",
    "                         num_workers=NUM_WORKERS,\n",
    "                         shuffle=False)\n",
    "\n",
    "print(train_loader)\n",
    "print(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modular/data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modular/data_setup.py\n",
    "\"\"\"\n",
    "Defines the functionality for creating PyTorch DataLoaders for the multi-class classification dataset.\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(train_dir: str, \n",
    "                       test_dir: str, \n",
    "                       transform: transforms.Compose, \n",
    "                       batch_size: int, \n",
    "                       num_workers: int=NUM_WORKERS):\n",
    "    \"\"\"Takes in a training and testing directory path and turns them into PyTorch DataLoaders.\n",
    "\n",
    "    Args:\n",
    "        train_dir (str): Path to training directory.\n",
    "        test_dir (str): Path to testing directory.\n",
    "        transform (transforms.Compose): Torchvision transforms to apply to the datasets.\n",
    "        batch_size (int): Number of samples per batch in each DataLoader.\n",
    "        num_workers (_type_): Number of workers per DataLoader. Currently set to os.cpu_count().\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Returns a tuple of (train_loader, test_loader, class_names). Where class_names is a dict of the target classes.\n",
    "    \"\"\"\n",
    "    # Use ImageFolder to create datasets\n",
    "    train_data = datasets.ImageFolder(root=train_dir,\n",
    "                                      transform=transform)\n",
    "    test_data = datasets.ImageFolder(root=test_dir,\n",
    "                                     transform=transform)\n",
    "    \n",
    "    # Get the class names\n",
    "    class_names = train_data.class_to_idx\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(dataset=train_data, \n",
    "                              batch_size=batch_size, \n",
    "                              num_workers=num_workers, \n",
    "                              shuffle=True,\n",
    "                              pin_memory=True)\n",
    "    \n",
    "    test_loader = DataLoader(dataset=test_data,\n",
    "                             batch_size=batch_size,\n",
    "                             num_workers=num_workers,\n",
    "                             shuffle=False,\n",
    "                             pin_memory=True)\n",
    "    \n",
    "    return train_loader, test_loader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Define the baseline model\n",
    "class BaseLine(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_shape, hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units, output_shape),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseLine(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=150528, out_features=10, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=10, out_features=8, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate the model\n",
    "torch.manual_seed(42)\n",
    "model = BaseLine(input_shape=224*224*3, \n",
    "                 hidden_units=10, \n",
    "                 output_shape=len(train_dataset.classes)).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single image shape: torch.Size([1, 3, 224, 224])\n",
      "\n",
      "Output logits:\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1466, 0.0979, 0.3001]],\n",
      "       device='cuda:0')\n",
      "\n",
      "Output prediction probabilities:\n",
      "tensor([[0.1161, 0.1161, 0.1161, 0.1161, 0.1161, 0.1345, 0.1281, 0.1568]],\n",
      "       device='cuda:0')\n",
      "\n",
      "Output prediction label:\n",
      "tensor([7], device='cuda:0')\n",
      "\n",
      "Actual label:\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Get a batch of images and labels from the DataLoader\n",
    "img_batch, label_batch = next(iter(train_loader))\n",
    "\n",
    "# Get a single images from the batch\n",
    "img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
    "print(f\"Single image shape: {img_single.shape}\\n\")\n",
    "\n",
    "# Perform a forward pass\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    pred = model(img_single.to(device))\n",
    "\n",
    "# Print out the conversion from logits, to pred probs, and pred label\n",
    "print(f\"Output logits:\\n{pred}\\n\")\n",
    "print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\n",
    "print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\n",
    "print(f\"Actual label:\\n{label_single}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modular/models/baseline_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modular/models/baseline_model.py\n",
    "\"\"\"\n",
    "Defines a PyTorch baseline model for multi-class classification.\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class BaseLine(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "        \"\"\"Defines a simple feedforward neural network for multi-class classification.\n",
    "\n",
    "        Args:\n",
    "            input_shape (int): Number of input channels.\n",
    "            hidden_units (int): Number of hidden units between layers.\n",
    "            output_shape (int): Number of output units.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_shape, hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units, output_shape),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modular/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modular/engine.py\n",
    "\"\"\"\n",
    "Defines functions for training and testing PyTorch models.\n",
    "\"\"\"\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"Turns the model into training mode and then runs through all the required training steps.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model to be trained.\n",
    "        dataloader (torch.utils.data.DataLoader): A DataLoader object for the training data.\n",
    "        loss_fn (torch.nn.Module): A PyTorch loss function to minimize.\n",
    "        optimizer (torch.optim.Optimizer): A PyTorch optimizer to update the model weights.\n",
    "        device (torch.device): A target device to send the data and model to.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: A tuple of the average loss and accuracy across all batches.\n",
    "    \"\"\"\n",
    "    # Put the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    #Setup the loss and accuracy\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # Iterate over the data\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send the data to the device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        y_pred = model(X)\n",
    "        \n",
    "        # Get the predictions\n",
    "        y_pred = model(X)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "        \n",
    "    # Adjust the metrics and get the avg loss and accuracy across all batches\n",
    "    tran_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"Turns the model into evaluation mode and then runs through all the required testing steps.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model to be tested.\n",
    "        dataloader (torch.utils.data.DataLoader): A DataLoader object for the testing data.\n",
    "        loss_fn (torch.nn.Module): A PyTorch loss function to minimize.\n",
    "        device (torch.device): A target device to send the data and model to.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: A tuple of the average loss and accuracy across all batches.\n",
    "    \"\"\"\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Setup the loss and accuracy\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    # Turn on inference mode\n",
    "    with torch.inference_mode():\n",
    "        # Iterate over the data\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send the data to the device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "        \n",
    "            # Forward pass\n",
    "            y_pred = model(X)\n",
    "        \n",
    "            # Calculate the loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "        \n",
    "            # Calculate and accumulate accuracy metric across all batches\n",
    "            y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "            test_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "        \n",
    "    # Adjust the metrics and get the avg loss and accuracy across all batches\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_loader: torch.utils.data.DataLoader,\n",
    "          test_loader: torch.utils.data.DataLoader,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          device: torch.device,\n",
    "          epochs: int) -> Dict[str, List[float]]:\n",
    "    \"\"\"Passes a model through training and testing steps for a specified number of epochs.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model to be trained and tested.\n",
    "        train_loader (torch.utils.data.DataLoader): A DataLoader object for the training data.\n",
    "        test_loader (torch.utils.data.DataLoader): A DataLoader object for the testing data.\n",
    "        loss_fn (torch.nn.Module): A PyTorch loss function to minimize.\n",
    "        optimizer (torch.optim.Optimizer): A PyTorch optimizer to update the model weights.\n",
    "        device (torch.device): A target device to send the data and model to.\n",
    "        epochs (int): The number of epochs to train the model for.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[float]]: A dictionary of lists containing the training and testing metrics.\n",
    "    \"\"\"\n",
    "    # Setup a dict to store results\n",
    "    results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # Perform a training step\n",
    "        train_loss, train_acc = train_step(model, train_loader, loss_fn, optimizer, device)\n",
    "        \n",
    "        # Perform a testing step\n",
    "        test_loss, test_acc = test_step(model, test_loader, loss_fn, device)\n",
    "        \n",
    "        # Append the metrics to the dict\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "        \n",
    "        # Print the metrics\n",
    "        print(f\"Epoch: {epoch+1}/{epochs} | Train Loss: {train_loss:.5f} | Train Acc: {train_acc:.5f} | Test Loss: {test_loss:.5f} | Test Acc: {test_acc:.5f}\")\n",
    "    \n",
    "    # Return the results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modular/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modular/utils.py\n",
    "\"\"\"\n",
    "Defines functions that contain various utility functions for PyTorch model training and saving.\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "def save_model(model: torch.nn.Module,\n",
    "               target_dir: str,\n",
    "               model_name: str) -> None:\n",
    "    \"\"\"Save a PyTorch model to a specified directory.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model to be saved.\n",
    "        target_dir (str): The directory path to save the model.\n",
    "        model_name (str): The name of the model file.\n",
    "    \"\"\"\n",
    "    # Create the target directory\n",
    "    Path(target_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create model save path\n",
    "    assert model_name.endswith(\".pt\") or model_name.endswith(\".pth\"), \"Model name must end with .pt or .pth\"\n",
    "    model_save_path = Path(target_dir) / model_name\n",
    "    \n",
    "    # Save the model\n",
    "    print(f\"Saving model to: {model_save_path}\")\n",
    "    torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modular/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modular/train.py\n",
    "\"\"\"\n",
    "Defines the training script for the PyTorch model.\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"modular\")\n",
    "sys.path.append(\"modular/models\")\n",
    "\n",
    "import data_setup, engine, baseline_model\n",
    "\n",
    "# Setup hyperparameters\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "HIDDEN_UNITS = 10\n",
    "\n",
    "# Setup the directories\n",
    "train_dir = \"data/training\"\n",
    "test_dir = \"data/testing\"\n",
    "\n",
    "# Setup target device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Setup the transformations\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create the DataLoaders using data_setup.py\n",
    "train_loader, test_loader, class_names = data_setup.create_dataloaders(train_dir, \n",
    "                                                                       test_dir, \n",
    "                                                                       data_transform, \n",
    "                                                                       BATCH_SIZE)\n",
    "\n",
    "# Create the model\n",
    "model = baseline_model.BaseLine(input_shape=224*224*3,\n",
    "                                hidden_units=HIDDEN_UNITS,\n",
    "                                output_shape=len(class_names)).to(device)\n",
    "\n",
    "# Set the loss function and optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Start training the model using engine.py\n",
    "engine.train(model, train_loader, test_loader, loss_fn, optimizer, device, NUM_EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ThirdProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
